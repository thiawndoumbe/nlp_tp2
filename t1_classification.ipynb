{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T√¢che #1 : Classification d'incidents avec des mod√®les *Transformers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On reprend, comme au premier travail, le th√®me de la classification de descriptions d‚Äôincidents. Cependant, la d√©finition des classes est diff√©rente pour ce travail et de nouveaux jeux de donn√©es ont √©t√© produits. Le corpus de textes contient 3 partitions : \n",
    "-\tUn fichier d‚Äôentra√Ænement -  *data/incidents_train.json*\n",
    "-\tUn fichier de validation -  *data/incidents_dev.json*\n",
    "-\tUn fichier de test - *data/incidents_test.json*\n",
    " \n",
    "Utilisez la librairie *HuggingFace* pour accomplir cette t√¢che. On vous demande plus sp√©cifiquement d‚Äôutiliser 2 mod√®les: le mod√®le *bert-base-uncased* et un autre mod√®le de votre choix. \n",
    "\n",
    "Les consignes pour cette t√¢che sont: \n",
    "- Nom du notebook : *t1_classification.ipynb* (ce notebook)\n",
    "- Tokenisation : Celle fournie par les tokeniseurs accompagnant les mod√®les transformers. \n",
    "- Plongements de mots : Ceux du mod√®le transformer. \n",
    "- Normalisation : Lettre en minuscule pour Bert (rien √† faire, le tokenizer s‚Äôen occupe). Aucune contrainte pour le 2e mod√®le mais il est pr√©f√©rable de ne pas alt√©rer le texte (sauf minuscule). \n",
    "- Choix du 2e transformer: Un mod√®le encodeur pr√©entra√Æn√© pour l‚Äôanglais ou multilingue. Le mod√®le ne doit pas √™tre une variante de Bert (p. ex. DistilBert). Me consulter en cas de doute.\n",
    "- Entra√Ænement : Un affinage (*fine-tuning*) du mod√®le, pas de pr√©entra√Ænement demand√© (*no further pretraining*). \n",
    "- Analyse : Pr√©sentez clairement vos r√©sultats et faites-en l‚Äôanalyse. Comparez les r√©sultats obtenus avec les 2 mod√®les.    \n",
    "\n",
    "Vous pouvez ajouter au *notebook* toutes les cellules dont vous avez besoin pour votre code, vos explications ou la pr√©sentation de vos r√©sultats. Vous pouvez √©galement ajouter des sous-sections (par ex. des sous-sections 1.1, 1.2 etc.) si cela am√©liore la lisibilit√©.\n",
    "\n",
    "Notes :\n",
    "- √âvitez les bouts de code trop longs ou trop complexes. Par exemple, il est difficile de comprendre 4-5 boucles ou conditions imbriqu√©es. Si c'est le cas, d√©finissez des sous-fonctions pour refactoriser et simplifier votre code. \n",
    "- Expliquez sommairement votre d√©marche.\n",
    "- Expliquez les choix que vous faites au niveau de la programmation et des mod√®les (si non trivial).\n",
    "- Analyser vos r√©sultats. Indiquez ce que vous observez, si c'est bon ou non, si c'est surprenant, etc. \n",
    "- Une analyse quantitative et qualitative d'erreurs est int√©ressante et permet de mieux comprendre le comportement d'un mod√®le."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cr√©ation du jeu de donn√©es (*les 3 partitions du dataset*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json_data(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notre collaboration avec Google Colab a √©t√© motiv√©e par le co√ªt √©lev√© de l'entra√Ænement des mod√®les transformers et la n√©cessit√© d'un GPU performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/Documents/ulaval/nlp/tp2/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data= {\"train\": \"incidents_train.json\", \"dev\": \"incidents_dev.json\",\"test\": \"incidents_test.json\"}\n",
    "data = load_dataset(\"json\", data_files=data, data_dir=r\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tokenizer et les embeddings de bert-base-uncased sont utilis√©s ici ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 531/531 [00:00<00:00, 9010.19 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' At approximately 8:50 a.m. on October 29  1997  Employee #1 was painting a  single story house at 2657 7th Ave  Sacramento  CA. He was caulking around the  peak of the roof line on the west side of the house  20 ft above the ground.  He was working off of a 24 ft aluminum extension ladder so that his feet were  approximately 12 to 13 feet above the ground. Employee #1 fell and suffered a  concussion and two dislocated discs in his lower back and was hospitalized.  The ladder was not secured to prevent movement.                                 ', 'label': '5', 'input_ids': [101, 2012, 3155, 1022, 1024, 2753, 1037, 1012, 1049, 1012, 2006, 2255, 2756, 2722, 7904, 1001, 1015, 2001, 4169, 1037, 2309, 2466, 2160, 2012, 20549, 2581, 5504, 13642, 11932, 6187, 1012, 2002, 2001, 6187, 5313, 6834, 2105, 1996, 4672, 1997, 1996, 4412, 2240, 2006, 1996, 2225, 2217, 1997, 1996, 2160, 2322, 3027, 2682, 1996, 2598, 1012, 2002, 2001, 2551, 2125, 1997, 1037, 2484, 3027, 13061, 5331, 10535, 2061, 2008, 2010, 2519, 2020, 3155, 2260, 2000, 2410, 2519, 2682, 1996, 2598, 1012, 7904, 1001, 1015, 3062, 1998, 4265, 1037, 23159, 1998, 2048, 4487, 14540, 24755, 3064, 15303, 1999, 2010, 2896, 2067, 1998, 2001, 24735, 1012, 1996, 10535, 2001, 2025, 7119, 2000, 4652, 2929, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def bert_preprocess_function(examples):\n",
    "    return bert_tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = data.map(bert_preprocess_function, batched=True)\n",
    "print(tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les labels doivent √™tre convertis en entiers pour l'entra√Ænement, sinon le format str pose un probl√®me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in tokenized_dataset.keys():\n",
    "    if 'label' in tokenized_dataset[partition].column_names:\n",
    "        labels = tokenized_dataset[partition]['label']\n",
    "        tokenized_dataset[partition] = tokenized_dataset[partition].remove_columns(\"label\")\n",
    "        tokenized_dataset[partition] = tokenized_dataset[partition].add_column(\"label\", [int(value) for value in labels])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour les r√©seaux r√©currents, il est n√©cessaire de faire un padding des s√©quences pour garantir une taille de tenseur √©quivalente pour chaque entr√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "dataset_collator = DataCollatorWithPadding(tokenizer=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cr√©ation des 2 mod√®les"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mod√®le BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les diff√©rentes fonctions qui seront utilis√©es pour √©valuer notre mod√®le sont implant√©es :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\", force_download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_predictions):\n",
    "    preds, true_labels = eval_predictions\n",
    "    predicted_label = np.argmax(preds, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predicted_label, references=true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici les classes sont converties en identifiants :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des dictionnaires de correspondance label <-> id\n",
    "nb_classes = len(np.unique(data['train']['label']))\n",
    "id2label = {i: str(i) for i in range(nb_classes)}\n",
    "label2id = {str(i): i for i in range(nb_classes)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisons le GPU  pour entra√Æner le mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device selected: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device selected: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification,TrainingArguments, Trainer\n",
    "\n",
    "bert_uncased_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=nb_classes,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Deuxi√®me mod√®le\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autre mod√®le pr√©-entra√Æn√© est utilis√© pour l'anglais : Electra.\n",
    "Le pr√©-entra√Ænement d'Electra diff√®re de celui de Bert. Effectivement, l'entra√Ænement pr√©liminaire de ce mod√®le repose sur une approche adversariale, ce qui permet de d√©terminer si des mots de la phrase sont des mots r√©els ou s'ils ont √©t√© g√©n√©r√©s par un mod√®le g√©n√©ratif. Il y a √©galement une diff√©rence entre Electra et Bert car le mod√®le ne fait pas de NSP pour son pr√©-entra√Ænement.\n",
    "\n",
    "Les m√™mes fonctions et donn√©es sont conserv√©es, le tokenizer et le mod√®le sont simplement remplac√©s par Electra :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_tokenizer = AutoTokenizer.from_pretrained(\"google/electra-base-discriminator\")\n",
    "\n",
    "def electra_preprocess_function(examples):\n",
    "    return electra_tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 531/531 [00:00<00:00, 10832.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "electra_tokenized_data = data.map(electra_preprocess_function, batched=True)\n",
    "for partition in electra_tokenized_data.keys():\n",
    "    labels = electra_tokenized_data[partition]['label']\n",
    "    electra_tokenized_data[partition] = electra_tokenized_data[partition].remove_columns(\"label\")\n",
    "    electra_tokenized_data[partition] = electra_tokenized_data[partition].add_column(\"label\", [int(label) for label in labels])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=electra_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "electra_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google/electra-base-discriminator\", \n",
    "    num_labels=nb_classes, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entra√Ænement des 2 mod√®les"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Mod√®le BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'entra√Ænement, nous avons choisi d'utiliser 3 √©poques afin d'obtenir de meilleures performances, bien que cela augmente le temps d'entra√Ænement en raison de la complexit√© du calcul d'attention. Les autres hyperparam√®tres restent ceux par d√©faut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/Documents/ulaval/nlp/tp2/venv/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_101672/3465564458.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_instance = Trainer(\n",
      "/home/jonathan/Documents/ulaval/nlp/tp2/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at ../aten/src/ATen/Context.cpp:296.)\n",
      "  return F.linear(input, self.weight, self.bias)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7425' max='7425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7425/7425 09:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.079400</td>\n",
       "      <td>1.020924</td>\n",
       "      <td>0.796610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.813900</td>\n",
       "      <td>0.933403</td>\n",
       "      <td>0.822976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.573300</td>\n",
       "      <td>1.043175</td>\n",
       "      <td>0.832392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7425, training_loss=0.8723962279040404, metrics={'train_runtime': 556.8123, 'train_samples_per_second': 13.335, 'train_steps_per_second': 13.335, 'total_flos': 555967530996570.0, 'train_loss': 0.8723962279040404, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"bert_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    "    )\n",
    "\n",
    "trainer_instance = Trainer(\n",
    "    model=bert_uncased_model,\n",
    "    args=training_params,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"dev\"],\n",
    "    tokenizer=bert_tokenizer,\n",
    "    data_collator=dataset_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "\n",
    ")\n",
    "\n",
    "trainer_instance.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Deuxi√®me mod√®le _ Electra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainement du model Electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/Documents/ulaval/nlp/tp2/venv/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_101672/4056485202.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_instance2 = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7425' max='7425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7425/7425 09:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.273400</td>\n",
       "      <td>1.064022</td>\n",
       "      <td>0.696798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.986100</td>\n",
       "      <td>0.986575</td>\n",
       "      <td>0.772128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.802900</td>\n",
       "      <td>1.093056</td>\n",
       "      <td>0.790960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7425, training_loss=1.069408538510101, metrics={'train_runtime': 561.0239, 'train_samples_per_second': 13.235, 'train_steps_per_second': 13.235, 'total_flos': 555967530996570.0, 'train_loss': 1.069408538510101, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_params2= TrainingArguments(\n",
    "    output_dir=\"model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    "    )\n",
    "\n",
    "trainer_instance2 = Trainer(\n",
    "    model=electra_model,\n",
    "    args=training_params2,\n",
    "    train_dataset=electra_tokenized_data[\"train\"],\n",
    "    eval_dataset=electra_tokenized_data[\"dev\"],\n",
    "    tokenizer=electra_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "\n",
    ")\n",
    "\n",
    "trainer_instance2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. √âvaluation, analyse de r√©sultats et comparaison des 2 mod√®les"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation  du model BERT\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 1.1154084205627441, 'test_accuracy': 0.775894538606403, 'test_runtime': 6.6417, 'test_samples_per_second': 79.949, 'test_steps_per_second': 79.949}\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer_instance.predict(tokenized_dataset[\"test\"])\n",
    "print(predictions[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation du model Electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 1.1527515649795532, 'test_accuracy': 0.743879472693032, 'test_runtime': 4.6396, 'test_samples_per_second': 114.45, 'test_steps_per_second': 114.45}\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer_instance2.predict(electra_tokenized_data[\"test\"])\n",
    "print(predictions[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyse de r√©sultats et comparaison des 2 mod√®les"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Precision (Precision)\n",
    "    BERT : Gr√¢ce √† une pr√©cision de 77,59% sur le jeu de test apr√®s trois √©poques, BERT met en √©vidence sa capacit√© √† identifier des relations complexes entre les mots dans le domaine textuel. Les r√©sultats de cette performance  montre que les t√™tes self-attention permettent vraiment au mod√®le de comprendre  les contructions et les liens entre les mots, ce qui renforce la capacit√© du mod√®le √† pr√©dire les classes et a saisir les d√©pendances √† long terme dans les descriptions des incidents. Cette pr√©cision est nettement sup√©rieure √† celle obtenue avec ELECTRA .\n",
    "\n",
    "    La pr√©cision du mod√®le ELECTRA est de 74,39%, ce qui, m√™me si elle est l√©g√®rement inf√©rieure √† celle de BERT, demeure comp√©titive. ELECTRA peut se concentrer plus rapidement sur les relations pertinentes dans les donn√©es gr√¢ce √† une strat√©gie de pr√©-entra√Ænement bas√©e sur la discrimination (au lieu du masquage comme pour BERT), ce qui peut am√©liorer son efficacit√© .\n",
    "\n",
    "    Conclusion sur la pr√©cision : Bien que BERT surpasse ELECTRA en termes de pr√©cision, ELECTRA reste performant et pourrait √™tre pr√©f√©r√© dans des cas o√π la vitesse et l'efficacit√© computationnelle sont des priorit√©s.\n",
    "\n",
    "2. Perte (Test Loss)\n",
    "    En ce qui concerne BERT, la perte de test de 1,115 est un indicateur stable de convergence, ce qui indique que le mod√®le s'est correctement adapt√© aux donn√©es lors de l'entra√Ænement. Cela laisse entendre que BERT peut g√©rer les cas complexes et r√©duire les erreurs de mani√®re plus efficace, m√™me si le nombre d'√©poques est relativement limit√©.\n",
    "\n",
    "\n",
    "    Conclusion sur la perte : BERT semble mieux g√©rer les relations complexes et converge plus efficacement dans ce cas sp√©cifique, tandis qu'ELECTRA, bien qu'efficace, n√©cessite peut-√™tre plus d'ajustements dans le fine-tuning.\n",
    "\n",
    "3. Vitesse d'Apprentissage et d'Inf√©rence\n",
    "    Vitesse d'apprentissage : Bien que ELECTRA soit r√©put√© pour son efficacit√© en fine-tuning gr√¢ce √† son approche de pr√©-entra√Ænement bas√©e sur la discrimination, dans ce cas, les r√©sultats ne montrent pas de diff√©rence notable en termes de vitesse d'apprentissage par rapport √† BERT. Cette √©galit√© sugg√®re que les deux mod√®les ont des performances d'entra√Ænement similaires avec les param√®tres choisis.\n",
    "\n",
    "    Vitesse d'inf√©rence : En revanche,  ELECTRA se d√©marque gr√¢ce √† sa rapidit√© d'inf√©rence. Son taux de traitement est de 114,45 √©chantillons par seconde, ce qui est pratiquement √† la fois plus rapide que BERT (qui traite 79,95 √©chantillons par seconde). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
